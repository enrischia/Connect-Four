{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/enrischia/Connect-Four/blob/main/Project_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spR-r4Jo1YMU"
      },
      "source": [
        "**Project**\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "**Track 1: Supervised Learning** \\\\\n",
        "Given a supervised dataset of game positions and moves selected by an AI agent, we aim to develop and train a machine learning model that imitates the moves performed by our agent, using four different approaches. In particular we decided to consider two different types of neural networks: A MultiLayer Percepton (MLP) and a Convolutional Neural Network (CNN). We also tried to train the models starting from two slighly different inputs. Firstly we used just two matrices (i.e. a tensor of size (2, 6, 7)) containing the positions of the two players on the board. Then, we added more features to include information on the location of empty cells and on some of the empty slots where either player could complete a line of four consecutive discs by placing a token."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqwr8Nij45n7"
      },
      "source": [
        "<br>\n",
        "\n",
        "**Settings**\n",
        "\n",
        "\n",
        "*   Import packages\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Laph71fs2prX"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import torch\n",
        "import gdown\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from sklearn import metrics\n",
        "from torchsummary import summary\n",
        "\n",
        "# Verify if GPU is available\n",
        "if torch.cuda.is_available():\n",
        "  device = 'cuda'\n",
        "else:\n",
        "  device = 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unw_pK0O4WsH"
      },
      "source": [
        "\n",
        "\n",
        "*   Function to process the imported files into list of pairs (position, move)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YPLZMA-q4mdv"
      },
      "outputs": [],
      "source": [
        "def process_dataset(filename):\n",
        "    dataset = []\n",
        "\n",
        "    with open(filename, 'r') as f:\n",
        "        # Initialize position with zero rows\n",
        "        position = []\n",
        "\n",
        "        # Initialize selected move\n",
        "        move = None\n",
        "\n",
        "        for line in f:\n",
        "            if len(position) < 6:\n",
        "                # This line describes a new row\n",
        "                position.append(list(line.strip('\\n')))\n",
        "\n",
        "            else:\n",
        "                # This line describes the move (an integer between 0 and 6)\n",
        "                move = int(line.strip())\n",
        "\n",
        "                # We need to reverse the rows in the position (the top row has index 5 but appeared first in the file)\n",
        "                position = list(reversed(position))\n",
        "\n",
        "                # Add (position, move) to the dataset\n",
        "                dataset.append((position, move))\n",
        "\n",
        "                # Reset position and move\n",
        "                position = []\n",
        "                move = None\n",
        "\n",
        "    print(f'Processed {filename} into a dataset with {len(dataset)} positions')\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykjdfXRz7sQ8"
      },
      "source": [
        "\n",
        "\n",
        "*   Functions to train and evaluate the model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ajidggc47zk7"
      },
      "outputs": [],
      "source": [
        "def training_procedure(model, optimizer, loader, epochs, loss_fn, validation_loader = None, cm = False, device = 'cpu', mu = 1/2, gamma = 0.1, test = False):\n",
        "\n",
        "    model.to(device) # Move the model to the specified device (CPU or GPU)\n",
        "\n",
        "    # Scheduler for learning rate adjustment\n",
        "    step_size = round(epochs * mu)  # Calculate step size for the reduced LR during training\n",
        "    scheduler = StepLR(optimizer, step_size = step_size, gamma = gamma, verbose = True)\n",
        "\n",
        "    # Initialize losses and accuracies lists\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "    train_accuracies = []\n",
        "\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        model.train() # Set the model to training mode\n",
        "\n",
        "        losses = []  # List to store batch losses during training\n",
        "        correct = 0  # Counter for correct predictions during training\n",
        "\n",
        "        for data, target in loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()  # Reset gradients\n",
        "            output = model(data)  # Forward pass\n",
        "            loss = loss_fn(output, target)  # Calculate loss\n",
        "            loss.backward()  # Backward pass\n",
        "            optimizer.step()  # Update weights\n",
        "\n",
        "            losses.append(loss.item())  # Track loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # Get predictions\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()  # Count correct predictions\n",
        "\n",
        "\n",
        "        # Calculate and store training metrics\n",
        "        avg_loss = np.mean(losses)\n",
        "        accuracy = 100. * correct / len(loader.dataset)\n",
        "        print(f'Train Epoch: {epoch + 1}  Loss: {avg_loss:.4f}')\n",
        "        train_losses.append(avg_loss)\n",
        "        train_accuracies.append(accuracy)\n",
        "\n",
        "        # Validation step\n",
        "        if validation_loader:\n",
        "            val_loss, val_accuracy = evaluating_procedure(model, validation_loader, loss_fn, cm, device)\n",
        "            val_losses.append(val_loss)\n",
        "            val_accuracies.append(val_accuracy)\n",
        "\n",
        "        scheduler.step() # Update learning rate\n",
        "        print(scheduler.get_last_lr())\n",
        "\n",
        "\n",
        "    # Plots: training vs. evaluation loss and accuracy\n",
        "    fig = plt.figure(figsize = (10, 6))\n",
        "    fig.subplots_adjust(wspace = 0.6)\n",
        "    #Plot losses\n",
        "    fig1 = fig.add_subplot(1, 2, 1)\n",
        "    fig1.plot(range(len(train_losses)), train_losses, label = 'Training loss', color = (97/255, 165/255, 194/255))\n",
        "    if validation_loader:\n",
        "      if test:\n",
        "        fig1.plot(range(len(train_losses)), val_losses, label = 'Test loss', color = (1/255, 58/255, 99/255))\n",
        "      else:\n",
        "        fig1.plot(range(len(train_losses)), val_losses, label = 'Validation loss', color = (1/255, 58/255, 99/255))\n",
        "    fig1.set_xlabel('Number of epochs')\n",
        "    fig1.set_ylabel('Cross-entropy loss')\n",
        "    if validation_loader:\n",
        "      if test:\n",
        "        fig1.set_title('Training loss vs. Test loss')\n",
        "      else:\n",
        "        fig1.set_title('Training loss vs. Validation loss')\n",
        "    else:\n",
        "      fig1.set_title('Training loss')\n",
        "    fig1.legend()\n",
        "    fig1.grid()\n",
        "    #Plot accuracies\n",
        "    fig2 = fig.add_subplot(1, 2, 2)\n",
        "    fig2.plot(range(len(train_losses)), train_accuracies, label = 'Training accuracy', color = (97/255, 165/255, 194/255))\n",
        "    if validation_loader:\n",
        "      if test:\n",
        "        fig2.plot(range(len(train_losses)), val_accuracies, label = 'Test accuracy', color = (1/255, 58/255, 99/255))\n",
        "      else:\n",
        "        fig2.plot(range(len(train_losses)), val_accuracies, label = 'Validation accuracy', color = (1/255, 58/255, 99/255))\n",
        "    fig2.set_xlabel('Number of epochs')\n",
        "    fig2.set_ylabel('Accuracy')\n",
        "    fig2.set_yticks([x for x in fig2.get_yticks()])\n",
        "    fig2.set_yticklabels([f'{x:.0f}%' for x in fig2.get_yticks()])\n",
        "    if validation_loader:\n",
        "      if test:\n",
        "        fig2.set_title('Training accuracy vs. Test accuracy')\n",
        "      else:\n",
        "        fig2.set_title('Training accuracy vs. Validation accuracy')\n",
        "    else:\n",
        "      fig2.set_title('Training accuracy')\n",
        "    fig2.legend()\n",
        "    fig2.grid()\n",
        "\n",
        "    fig.show()\n",
        "    return\n",
        "\n",
        "\n",
        "def evaluating_procedure(model, loader, loss_fn, cm = False, device = 'cpu'):\n",
        "\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    model.to(device)  # Move model to the specified device\n",
        "\n",
        "    losses = 0\n",
        "    correct = 0\n",
        "    targets = []\n",
        "    preds = []\n",
        "\n",
        "    with torch.no_grad(): # Disable gradient computation\n",
        "        for data, target in loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)  # Forward pass\n",
        "            losses += loss_fn(output, target).item()  # Accumulate loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # Get predictions\n",
        "            preds.append(pred.tolist())  # Store predictions\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()  # Count correct predictions\n",
        "            targets.append(target.view_as(pred).tolist())  # Store true labels\n",
        "\n",
        "    # Calculate average loss and accuracy\n",
        "    losses /= len(loader)\n",
        "    accuracy = 100. * correct / len(loader.dataset)\n",
        "    print('Evaluation set: Average loss: %.4f, Accuracy: %d/%d (%.4f)' % (losses, correct, len(loader.dataset), accuracy))\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    if cm:\n",
        "      targets_flat = [x_ for xs in targets for x in xs for x_ in x]\n",
        "      preds_flat = [x_ for xs in preds for x in xs for x_ in x]\n",
        "      confusion_matrix = metrics.confusion_matrix(targets_flat, preds_flat)\n",
        "      cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [0, 1, 2, 3, 4, 5, 6])\n",
        "      cm_display.plot(cmap ='PuBu')\n",
        "      plt.title('Confusion matrix', fontsize = 17)\n",
        "      plt.show()\n",
        "\n",
        "      confusion_matrix_norm = metrics.confusion_matrix(targets_flat, preds_flat, normalize = 'true')\n",
        "      cm_display_norm = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix_norm, display_labels = [0, 1, 2, 3, 4, 5, 6])\n",
        "      cm_display_norm.plot(cmap = 'PuBu')\n",
        "      plt.title('     normalization over rows')\n",
        "      plt.suptitle('Recall (on the main diagonal)', fontsize = 17, y = 1)\n",
        "      plt.show()\n",
        "\n",
        "      confusion_matrix_norm = metrics.confusion_matrix(targets_flat, preds_flat, normalize = 'pred')\n",
        "      cm_display_norm = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix_norm, display_labels = [0, 1, 2, 3, 4, 5, 6])\n",
        "      cm_display_norm.plot(cmap = 'PuBu')\n",
        "      plt.title('     normalization over columns')\n",
        "      plt.suptitle('Precision (on the main diagonal)', fontsize = 17, y = 1)\n",
        "      plt.show()\n",
        "\n",
        "    return losses, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YTUPtDxa-kw"
      },
      "source": [
        "\n",
        "\n",
        "*   Function that creates an iterable dataset object in batches of a given input batch size over the training and validation sets\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qgJAICI2vgZ8"
      },
      "outputs": [],
      "source": [
        "def create_data(train_set, val_set, batch_size):\n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(train_set, batch_size = batch_size, shuffle = True)\n",
        "  val_loader = torch.utils.data.DataLoader(val_set, batch_size = batch_size, shuffle = True)\n",
        "\n",
        "  return train_loader, val_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Function that identifies vertical, horizontal or diagonal lines of three consecutive discs in a given input grid (of either yellow or red tokens) and returns a 6x7 matrix marking all the corresponding empty slots in the grid that would allow a player to form a winning sequence of four discs"
      ],
      "metadata": {
        "id": "SdSR4JGwOMIS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5yqwe3HXGP0I"
      },
      "outputs": [],
      "source": [
        "def three_consecuitive_matrix(positions, positions_empty):\n",
        "\n",
        "  # Define length of the input tensor\n",
        "  l = len(positions)\n",
        "\n",
        "  # Define kernels to detect three consecutive tokens\n",
        "  horizontal_kernel = torch.tensor([[[[1, 1, 1]]]], dtype = torch.float32)\n",
        "  vertical_kernel = torch.tensor([[[[1], [1], [1]]]], dtype = torch.float32)\n",
        "  diag1_kernel = torch.tensor([[[[1, 0, 0], [0, 1, 0], [0, 0, 1]]]], dtype = torch.float32)\n",
        "  diag2_kernel = torch.tensor([[[[0, 0, 1], [0, 1, 0], [1, 0, 0]]]], dtype = torch.float32)\n",
        "\n",
        "  # Add channel dimension\n",
        "  positions = positions.unsqueeze(1)  # shape: (l, 1, 6, 7)\n",
        "\n",
        "  # Perform convolutions to find three consecutive tokens\n",
        "  horizontal_output = F.conv2d(positions, horizontal_kernel, padding = (0, 1))\n",
        "  vertical_output = F.conv2d(positions, vertical_kernel, padding = (1, 0))\n",
        "  diag1_output = F.conv2d(positions, diag1_kernel, padding = 1)\n",
        "  diag2_output = F.conv2d(positions, diag2_kernel, padding = 1)\n",
        "\n",
        "  # Create masks to find potential positions to complete a line of four tokens\n",
        "  horizontal_positions = (horizontal_output == 3).float()\n",
        "  vertical_positions = (vertical_output == 3).float()\n",
        "  diag1_positions = (diag1_output == 3).float()\n",
        "  diag2_positions = (diag2_output == 3).float()\n",
        "\n",
        "  # Generate horizontal candidates\n",
        "  horizontal_candidates1 = torch.cat((torch.zeros(l,1,6,2), horizontal_positions[:, :, :, :-2]), 3)\n",
        "  horizontal_candidates2 = torch.cat((horizontal_positions[:, :, :, 2:], torch.zeros(l,1,6,2)), 3)\n",
        "\n",
        "  # Generate vertical candidates\n",
        "  vertical_candidates1 = torch.cat((torch.zeros(l,1,2,7), vertical_positions[:, :, :-2, :]), 2)\n",
        "  vertical_candidates2 = torch.cat((vertical_positions[:, :, 2:, :], torch.zeros(l,1,2,7)), 2)\n",
        "\n",
        "  # Generate diagonal candidates (\\)\n",
        "  diag1_candidates1 = torch.cat((torch.zeros(l,1,2,7), torch.cat((torch.zeros(l,1,4,2), diag1_positions[:, :, :-2, :-2]), 3)), 2)\n",
        "  diag1_candidates2 = torch.cat((torch.cat((diag1_positions[:, :, 2:, 2:], torch.zeros(l,1,4,2)), 3), torch.zeros(l,1,2,7)), 2)\n",
        "\n",
        "  # Generate diagonal candidates (/)\n",
        "  diag2_candidates1 = torch.cat((torch.cat((torch.zeros(l,1,4,2), diag2_positions[:, :, 2:, :-2]), 3), torch.zeros(l,1,2,7)), 2)\n",
        "  diag2_candidates2 = torch.cat((torch.zeros(l,1,2,7), torch.cat((diag2_positions[:, :, :-2, 2:], torch.zeros(l,1,4,2)), 3)), 2)\n",
        "\n",
        "  # Combine all potential positions\n",
        "  potential_positions = (horizontal_candidates1 + horizontal_candidates2 + vertical_candidates1 + vertical_candidates2+ diag1_candidates1 + diag1_candidates2 +\n",
        "                        diag2_candidates1 + diag2_candidates2) > 0\n",
        "\n",
        "  potential_positions = potential_positions.squeeze(1)\n",
        "  positions = positions.squeeze(1)\n",
        "\n",
        "  # Ensure that the potential positions are valid (i.e. empty slots in the current grid)\n",
        "  result_positions = potential_positions.float() * (positions_empty == 1).float()\n",
        "\n",
        "  return result_positions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Function that removes duplicates from the training dataset and enlarges it including also the symmetric version of the board, if not already present"
      ],
      "metadata": {
        "id": "6t_cnuUyM2J_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "IkOm1xU-nYor"
      },
      "outputs": [],
      "source": [
        "def remove_duplicates(train_set):\n",
        "\n",
        "    # Create a list to store unique elements\n",
        "    unique_elements = []\n",
        "\n",
        "    for element in train_set:\n",
        "        # Check if the element is already in the unique list\n",
        "        if element not in unique_elements:\n",
        "            unique_elements.append(element)  # Add the element to the unique list\n",
        "\n",
        "            # Create the symmetric version of the board\n",
        "            symmetric_board = [row[::-1] for row in element[0]]\n",
        "            symmetric_element = (symmetric_board, 6 - element[1])\n",
        "\n",
        "            # Add the symmetric element to the unique list if not already present\n",
        "            if symmetric_element not in unique_elements:\n",
        "                unique_elements.append(symmetric_element)\n",
        "\n",
        "    return unique_elements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BL4A5jZ65UQL"
      },
      "source": [
        "**Import and process the data**\n",
        "*   Import train and test datasets as text files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "id": "uL6mGI_t6CVG",
        "outputId": "4f3615cb-9dd6-48dc-aeef-1dc64e4d931d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1zwshKNoeCh_JHYbaDS0e5Mu_kTPXfL_y\n",
            "To: /content/train_set.txt\n",
            "100%|██████████| 350k/350k [00:00<00:00, 39.3MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Y9HM-56TTDhIq9B1w7H_WwN1Htg87LyY\n",
            "To: /content/test_set.txt\n",
            "100%|██████████| 50.0k/50.0k [00:00<00:00, 59.5MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'test_set.txt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "train_set_url = 'https://drive.google.com/file/d/1zwshKNoeCh_JHYbaDS0e5Mu_kTPXfL_y/view?usp=drive_link'\n",
        "train_set_filename = 'train_set.txt'\n",
        "\n",
        "gdown.download(train_set_url, train_set_filename, fuzzy=True)\n",
        "\n",
        "test_set_url = 'https://drive.google.com/file/d/1Y9HM-56TTDhIq9B1w7H_WwN1Htg87LyY/view?usp=drive_link'\n",
        "test_set_filename = 'test_set.txt'\n",
        "\n",
        "gdown.download(test_set_url, test_set_filename, fuzzy=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJOz-5BI6VBp"
      },
      "source": [
        "*   Convert the datasets into PyTorch datasets, extending them by considering also the symmetric version of each grid and extract different features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYk4KdNV6eYg",
        "outputId": "9d2a7662-db49-4103-8dd3-59ef45a26bb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed train_set.txt into a dataset with 7000 positions\n",
            "Processed test_set.txt into a dataset with 1000 positions\n"
          ]
        }
      ],
      "source": [
        "# 1. Use the function \"process_dataset\" to convert the datasets from text files into a list\n",
        "#    of tuples, each one containing a list of lists that represents the grid of\n",
        "#    the match and the associated move\n",
        "\n",
        "train_set = process_dataset(train_set_filename)\n",
        "test_set = process_dataset(test_set_filename)\n",
        "\n",
        "\n",
        "# 2. Narrow down the increased dataset (with all the flipped elements) by removing all duplicates\n",
        "\n",
        "unique = remove_duplicates(train_set)\n",
        "\n",
        "\n",
        "# 3. Use the build-in function zip and the * operator to unzip the lists\n",
        "#    and create two different tuples, one for the match positions and one for the moves\n",
        "\n",
        "positions_train, moves_train = zip(*unique)\n",
        "positions_test, moves_test = zip(*test_set)\n",
        "\n",
        "\n",
        "# 4. Starting from the match grids create three different 6x7 matrices:\n",
        "#    one marking the positions of red tokens, one for the yellow ones and one for the empty slots\n",
        "\n",
        "positions_red_train = torch.tensor([[[1 if elem == 'X' else 0 for elem in row] for row in position] for position in positions_train]).float()\n",
        "positions_yel_train = torch.tensor([[[1 if elem == 'O' else 0 for elem in row] for row in position] for position in positions_train]).float()\n",
        "positions_emp_train = torch.tensor([[[1 if elem == ' ' else 0 for elem in row] for row in position] for position in positions_train]).float()\n",
        "\n",
        "positions_red_test = torch.tensor([[[1 if elem == 'X' else 0 for elem in row] for row in position] for position in positions_test]).float()\n",
        "positions_yel_test = torch.tensor([[[1 if elem == 'O' else 0 for elem in row] for row in position] for position in positions_test]).float()\n",
        "positions_emp_test = torch.tensor([[[1 if elem == ' ' else 0 for elem in row] for row in position] for position in positions_test]).float()\n",
        "\n",
        "#    Create two more matrices using the function \"three_consecutive_matrix\" defined above\n",
        "three_red_train = three_consecuitive_matrix(positions_red_train, positions_emp_train)\n",
        "three_yel_train = three_consecuitive_matrix(positions_yel_train, positions_emp_train)\n",
        "\n",
        "three_red_test = three_consecuitive_matrix(positions_red_test, positions_emp_test)\n",
        "three_yel_test = three_consecuitive_matrix(positions_yel_test, positions_emp_test)\n",
        "\n",
        "\n",
        "# 5. Concatenate the position matrices in a tensor\n",
        "\n",
        "stacked_inputs_ry = torch.stack((positions_red_train, positions_yel_train), dim = 1) # tensor of size (2,6,7) containing information on red and yellow tokens\n",
        "stacked_inputs_ryery = torch.stack((positions_red_train, positions_yel_train, positions_emp_train, three_red_train, three_yel_train ), dim = 1) # tensor of size (5, 6, 7) containg all the information\n",
        "\n",
        "stacked_test_ry = torch.stack((positions_red_test, positions_yel_test), dim = 1)\n",
        "stacked_test_ryery = torch.stack((positions_red_test, positions_yel_test, positions_emp_test, three_red_test, three_yel_test ), dim = 1)\n",
        "\n",
        "\n",
        "# 6. Convert into PyTorch datasets\n",
        "\n",
        "pytorch_train_ry = torch.utils.data.TensorDataset(stacked_inputs_ry, torch.tensor(moves_train))\n",
        "pytorch_train_ryery = torch.utils.data.TensorDataset(stacked_inputs_ryery, torch.tensor(moves_train))\n",
        "\n",
        "pytorch_test_ry = torch.utils.data.TensorDataset(stacked_test_ry, torch.tensor(moves_test))\n",
        "pytorch_test_ryery = torch.utils.data.TensorDataset(stacked_test_ryery, torch.tensor(moves_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUzRjDnk7SPf"
      },
      "source": [
        "*   Split the dataset into training and validation sets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UwZ2bBD7ZT1"
      },
      "outputs": [],
      "source": [
        "train_ry, val_ry = torch.utils.data.random_split(pytorch_train_ry, [11878, 1000])\n",
        "train_ryery, val_ryery = torch.utils.data.random_split(pytorch_train_ryery, [11878, 1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhdU88OuOa6C"
      },
      "source": [
        "<br><br>\n",
        "\n",
        "---\n",
        "<br>\n",
        "\n",
        "**Fully connected neural networks**\n",
        "\n",
        "**1.**   Considering the dataset with no repetition and two imputs matrices corresponding to the red and yellow tokens in the match grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9m0quXzZo_n"
      },
      "outputs": [],
      "source": [
        "fullyconnected_model_ry = nn.Sequential(\n",
        "\n",
        "    nn.Flatten(),             # Flatten the input (transform a multi-dimensional array into a one-dimensional array)\n",
        "\n",
        "    nn.Linear(42*2, 350),     # Linear layer with input size 84 (42*2)\n",
        "    nn.BatchNorm1d(350),      # Normalize the input values ​​in each mini-batch, so that they have a mean of zero and a standard deviation of one, to improve training stability\n",
        "    nn.ReLU(),                # ReLU activation function\n",
        "    nn.Dropout(0.5),          # Dropout with 0.5 probability to reduce overfitting\n",
        "\n",
        "    nn.Linear(350, 150),      # Linear layer\n",
        "    nn.BatchNorm1d(150),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "\n",
        "    nn.Linear(150, 7),        # Linear layer, 7-class classification\n",
        "\n",
        ")\n",
        "\n",
        "if device == 'cuda':\n",
        "  summary(fullyconnected_model_ry.cuda(), (2,6,7))\n",
        "else:\n",
        "  summary(fullyconnected_model_ry, (2,6,7))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRrLEBxXac4K"
      },
      "source": [
        "<br>\n",
        "\n",
        "\\- Model training\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENLJS1g7QmCZ"
      },
      "outputs": [],
      "source": [
        "# 1. Create an iterable object in batches of both train and validation sets\n",
        "\n",
        "batch_size_ry_mlp = 20\n",
        "train_loader_ry_mlp, val_loader_ry_mlp = create_data(train_ry, val_ry, batch_size_ry_mlp)\n",
        "\n",
        "\n",
        "# 2. Set hyperparameters and functions for the training procedure: learning rate, multiplicative learning rate decay factor gamma,\n",
        "#    percentage of learning rate decay mu, number of epochs, weight decay (regularization parameter), optimizer and loss function\n",
        "\n",
        "learning_rate_ry_mlp = 0.0005\n",
        "mu_ry_mlp = 1/2\n",
        "gamma_ry_mlp = 0.2\n",
        "epochs_ry_mlp = 25\n",
        "weight_decay_ry_mlp = 5e-3\n",
        "optimizer_ry_mlp = torch.optim.Adam(fullyconnected_model_ry.parameters(), lr = learning_rate_ry_mlp,\n",
        "                                           weight_decay = weight_decay_ry_mlp)\n",
        "loss_fn_ry_mlp = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# # 3. Training of the model\n",
        "\n",
        "# training_procedure(fullyconnected_model_ry, optimizer_ry_mlp, train_loader_ry_mlp, epochs_ry_mlp, loss_fn_ry_mlp,\n",
        "#                    validation_loader = val_loader_ry_mlp, cm = False, device = device, mu = mu_ry_mlp, gamma = gamma_ry_mlp)\n",
        "\n",
        "\n",
        "# # 4. Evaluation of the model on the validation set\n",
        "\n",
        "# __ = evaluating_procedure(fullyconnected_model_ry, val_loader_ry_mlp, loss_fn_ry_mlp, cm = True, device = device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jrs4Uu775mbv"
      },
      "source": [
        "<br>\n",
        "\n",
        "\\- Retrain the model on the whole training dataset and evaluate it on the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrFYDI8XuGzM",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "fullyconnected_test_ry = nn.Sequential(\n",
        "\n",
        "    nn.Flatten(),             # Flatten the input (transform a multi-dimensional array into a one-dimensional array)\n",
        "\n",
        "    nn.Linear(42*2, 350),     # Linear layer with input size 84 (42*2)\n",
        "    nn.BatchNorm1d(350),      # Normalize the input values ​​in each mini-batch, so that they have a mean of zero and a standard deviation of one, to improve training stability\n",
        "    nn.ReLU(),                # ReLU activation function\n",
        "    nn.Dropout(0.5),          # Dropout with 0.5 probability to reduce overfitting\n",
        "\n",
        "    nn.Linear(350, 150),      # Linear layer\n",
        "    nn.BatchNorm1d(150),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "\n",
        "    nn.Linear(150, 7),        # Linear layer, 7-class classification\n",
        ")\n",
        "\n",
        "epochs_ry_mlp = 45\n",
        "train_loader_ry_mlp, test_loader_ry_mlp = create_data(pytorch_train_ry, pytorch_test_ry, batch_size_ry_mlp)\n",
        "\n",
        "optimizer_ry_mlp = torch.optim.Adam(fullyconnected_test_ry.parameters(), lr = learning_rate_ry_mlp,\n",
        "                                           weight_decay = weight_decay_ry_mlp)\n",
        "loss_fn_ry_mlp = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "\n",
        "training_procedure(fullyconnected_test_ry, optimizer_ry_mlp, train_loader_ry_mlp, epochs_ry_mlp, loss_fn_ry_mlp,\n",
        "                   validation_loader = test_loader_ry_mlp, cm = False, device = device, mu = mu_ry_mlp, gamma = gamma_ry_mlp, test = True)\n",
        "\n",
        "__ = evaluating_procedure(fullyconnected_test_ry, test_loader_ry_mlp, loss_fn_ry_mlp, cm = True, device = device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEAKYgQ7mAse"
      },
      "source": [
        "<br> <br>\n",
        "\n",
        "**2.** Considering the dataset with no repetition and five imputs matrices representing the Red and Yellow positions in the match grid, the matrix of Empty cells and the matrices created via the \"three_consecutive_matrix\" function applied respectively to the Red token grid and the Yellow ones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7L4r0JxUmJem"
      },
      "outputs": [],
      "source": [
        "fullyconnected_model_ryery = nn.Sequential(\n",
        "\n",
        "    nn.Flatten(),             # Flatten the input (transforms a multi-dimensional array into a one-dimensional array)\n",
        "\n",
        "    nn.Linear(42*5, 350),     # Linear layer with input size 84 (42*2)\n",
        "    nn.BatchNorm1d(350),      # Normalize the input values ​​in each mini-batch, so that they have a mean of zero and a standard deviation of one, to improve training stability\n",
        "    nn.ReLU(),                # ReLU activation function\n",
        "    nn.Dropout(0.5),          # Dropout with 0.5 probability to reduce overfitting\n",
        "\n",
        "    nn.Linear(350, 128),      # Linear layer\n",
        "    nn.BatchNorm1d(128),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "\n",
        "    nn.Linear(128, 64),      # Linear layer\n",
        "    nn.BatchNorm1d(64),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "\n",
        "    nn.Linear(64, 7),        # Linear layer, 7-class classification\n",
        ")\n",
        "\n",
        "\n",
        "print(fullyconnected_model_ryery)\n",
        "if device == 'cuda':\n",
        "  summary(fullyconnected_model_ryery.cuda(), (5,6,7))\n",
        "else:\n",
        "  summary(fullyconnected_model_ryery, (5,6,7))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "\\- Model training"
      ],
      "metadata": {
        "id": "a2IoT-lLea0C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tW2eIfpmTq_"
      },
      "outputs": [],
      "source": [
        "# 1. Create an iterable object in batches of both train and validation sets\n",
        "\n",
        "batch_size_ryery_mlp = 20\n",
        "train_loader_ryery_mlp, val_loader_ryery_mlp = create_data(train_ryery, val_ryery, batch_size_ryery_mlp)\n",
        "\n",
        "\n",
        "# 2. Set hyperparameters and functions for the training procedure\n",
        "\n",
        "learning_rate_ryery_mlp = 0.0001\n",
        "mu_ryery_mlp = 5/8\n",
        "epochs_ryery_mlp = 60\n",
        "weight_decay_ryery_mlp = 5e-3\n",
        "gamma_ryery_mlp = 0.3\n",
        "optimizer_ryery_mlp = torch.optim.Adam(fullyconnected_model_ryery.parameters(), lr = learning_rate_ryery_mlp,\n",
        "                                           weight_decay = weight_decay_ryery_mlp)\n",
        "loss_fn_ryery_mlp = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# # 3. Training of the model\n",
        "\n",
        "# training_procedure(fullyconnected_model_ryery, optimizer_ryery_mlp, train_loader_ryery_mlp, epochs_ryery_mlp,\n",
        "#                    loss_fn_ryery_mlp, validation_loader = val_loader_ryery_mlp, cm = False, device = device, mu = mu_ryery_mlp, gamma = gamma_ryery_mlp)\n",
        "\n",
        "\n",
        "# # 4. Evaluation of the model on the validation set\n",
        "\n",
        "# __ = evaluating_procedure(fullyconnected_model_ryery, val_loader_ryery_mlp, loss_fn_ryery_mlp, cm = True, device = device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "\\- Retrain the model on the whole training dataset and evaluate it on the test set"
      ],
      "metadata": {
        "id": "XPJBj1zbmhWD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBddc4U_syc5"
      },
      "outputs": [],
      "source": [
        "fullyconnected_test_ryery = nn.Sequential(\n",
        "\n",
        "    nn.Flatten(),             # Flatten the input (transforms a multi-dimensional array into a one-dimensional array)\n",
        "\n",
        "    nn.Linear(42*5, 350),     # Linear layer with input size 84 (42*2)\n",
        "    nn.BatchNorm1d(350),      # Normalize the input values ​​in each mini-batch, so that they have a mean of zero and a standard deviation of one, to improve training stability\n",
        "    nn.ReLU(),                # ReLU activation function\n",
        "    nn.Dropout(0.5),          # Dropout with 0.5 probability to reduce overfitting\n",
        "\n",
        "    nn.Linear(350, 128),      # Linear layer\n",
        "    nn.BatchNorm1d(128),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "\n",
        "    nn.Linear(128, 64),      # Linear layer\n",
        "    nn.BatchNorm1d(64),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "\n",
        "    nn.Linear(64, 7),        # Linear layer, 7-class classification\n",
        ")\n",
        "\n",
        "epochs_ryery_mlp = 80\n",
        "train_loader_ryery_mlp, test_loader_ryery_mlp = create_data(pytorch_train_ryery, pytorch_test_ryery, batch_size_ryery_mlp)\n",
        "\n",
        "optimizer_ryery_mlp = torch.optim.Adam(fullyconnected_test_ryery.parameters(), lr = learning_rate_ryery_mlp,\n",
        "                                           weight_decay = weight_decay_ryery_mlp)\n",
        "loss_fn_ryery_mlp = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "\n",
        "training_procedure(fullyconnected_test_ryery, optimizer_ryery_mlp, train_loader_ryery_mlp, epochs_ryery_mlp,\n",
        "                   loss_fn_ryery_mlp, validation_loader = test_loader_ryery_mlp, cm = False, device = device, mu = mu_ryery_mlp, gamma = gamma_ryery_mlp, test = True)\n",
        "\n",
        "__ = evaluating_procedure(fullyconnected_test_ryery, test_loader_ryery_mlp, loss_fn_ryery_mlp, cm = True, device = device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woaloaLrhUYJ"
      },
      "source": [
        "<br><br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "**Convolutional Neural Networks**\n",
        "\n",
        "**1.**   Considering the dataset with no repetition and two imputs matrices corresponding to Red and Yellow positions in the match grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DJyo1X-thL-"
      },
      "outputs": [],
      "source": [
        "# cnn_model_ry = nn.Sequential(\n",
        "\n",
        "#     #Kernel: a small matrix used in convolution to capture certain patterns\n",
        "#     #Padding to add extra pixels around the edges of the input image\n",
        "\n",
        "#     nn.Conv2d(2, 64, kernel_size =4, padding=2),     # Convolutional layer with 2 input matrices\n",
        "#     nn.BatchNorm2d(64),                              # Normalize the input values ​​in each mini-batch, so that they have a mean of zero and a standard deviation of one\n",
        "#     nn.ReLU(),                                       # ReLU activation function\n",
        "#     nn.Dropout(0.15),\n",
        "\n",
        "#     nn.Conv2d(64, 32, kernel_size=4, padding=1),     # Convolutional layer\n",
        "#     nn.BatchNorm2d(32),\n",
        "#     nn.ReLU(),\n",
        "#     nn.Dropout(0.3),\n",
        "\n",
        "#     nn.Conv2d(32, 32, kernel_size=4, padding=1),     # Convolutional layer\n",
        "#     nn.BatchNorm2d(32),\n",
        "#     nn.ReLU(),\n",
        "#     nn.Dropout(0.4),\n",
        "\n",
        "#     nn.Conv2d(32, 16, kernel_size=3, padding=1),     # Convolutional layer\n",
        "#     nn.BatchNorm2d(16),\n",
        "#     nn.ReLU(),\n",
        "#     nn.Dropout(0.4),\n",
        "\n",
        "#     nn.Flatten(),                                    # Flatten the input\n",
        "\n",
        "#     nn.Linear(480, 64),                              # Linear layer\n",
        "#     nn.BatchNorm1d(64),\n",
        "#     nn.ReLU(),\n",
        "#     nn.Dropout(0.5),\n",
        "\n",
        "#     nn.Linear(64, 7),                                # Linear layer\n",
        "\n",
        "# )\n",
        "\n",
        "# if device == 'cuda':\n",
        "#   summary(cnn_model_ry.cuda(), (2,6,7))\n",
        "# else:\n",
        "#   summary(cnn_model_ry, (2,6,7))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "\\- Model training"
      ],
      "metadata": {
        "id": "m0ouM1oXmt1F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4h75rwJtz6H"
      },
      "outputs": [],
      "source": [
        "# # 1. Compute the batch of both train and validation sets\n",
        "\n",
        "# batch_size_ry_cnn=30\n",
        "# train_loader_ry_cnn, val_loader_ry_cnn= create_data(train_ry, val_ry, batch_size_ry_cnn)\n",
        "\n",
        "\n",
        "# # 2. Set parameters and functions for the training procedure such as: learning rate, multiplicative learning rate decay factor gamma,\n",
        "# #    percentage of learning rate decay mu, number of epochs, weight decay (regularization parameter), optimizer and loss function\n",
        "\n",
        "# learning_rate_ry_cnn = 0.005\n",
        "# epochs_ry_cnn = 120\n",
        "# weight_decay_ry_cnn = 1e-3\n",
        "# optimizer_ry_cnn = torch.optim.Adam(cnn_model_ry.parameters(), lr = learning_rate_ry_cnn, weight_decay = weight_decay_ry_cnn)\n",
        "# loss_fn_ry_cnn = nn.CrossEntropyLoss()\n",
        "# mu_ry_cnn = 2/3\n",
        "\n",
        "\n",
        "# # 3. Training of the model\n",
        "\n",
        "# train_losses = training_procedure(cnn_model_ry, optimizer_ry_cnn, train_loader_ry_cnn, epochs_ry_cnn,\n",
        "#                                   loss_fn_ry_cnn, validation_loader = val_loader_ry_cnn, device = device, mu = mu_ry_cnn)\n",
        "\n",
        "\n",
        "# # 4. Evaluation of the model in the validation set\n",
        "\n",
        "# __ = evaluating_procedure(cnn_model_ry, val_loader_ry_cnn, loss_fn_ry_cnn, cm = True, device = device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "\\- Retrain the model on the whole training dataset and evaluate it on the test set"
      ],
      "metadata": {
        "id": "nW_EJyHApKw9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nHVSq4qkVhh"
      },
      "outputs": [],
      "source": [
        "# cnn_test_ry = nn.Sequential(\n",
        "\n",
        "#     #Kernel a small matrix used in convolution to capture certain patterns\n",
        "#     #Padding to add extra pixels around the edges of the input image\n",
        "\n",
        "#     nn.Conv2d(2, 64, kernel_size =4, padding=2),     # Convolutional layer with 2 input matrices\n",
        "#     nn.BatchNorm2d(64),                              # Normalizes the input values ​​in each mini-batch, so that they have a mean of zero and a standard deviation of one\n",
        "#     nn.ReLU(),                                       # ReLU activation function\n",
        "#     nn.Dropout(0.15),\n",
        "\n",
        "#     nn.Conv2d(64, 32, kernel_size=4, padding=1),     # Convolutional layer\n",
        "#     nn.BatchNorm2d(32),\n",
        "#     nn.ReLU(),\n",
        "#     nn.Dropout(0.3),\n",
        "\n",
        "#     nn.Conv2d(32, 32, kernel_size=4, padding=1),     # Convolutional layer\n",
        "#     nn.BatchNorm2d(32),\n",
        "#     nn.ReLU(),\n",
        "#     nn.Dropout(0.4),\n",
        "\n",
        "#     nn.Conv2d(32, 16, kernel_size=3, padding=1),     # Convolutional layer\n",
        "#     nn.BatchNorm2d(16),\n",
        "#     nn.ReLU(),\n",
        "#     nn.Dropout(0.4),\n",
        "\n",
        "#     nn.Flatten(),                                    # Flattens the input\n",
        "\n",
        "#     nn.Linear(480, 64),                              # Linear layer\n",
        "#     nn.BatchNorm1d(64),\n",
        "#     nn.ReLU(),\n",
        "#     nn.Dropout(0.5),\n",
        "\n",
        "#     nn.Linear(64, 7),                                # Linear layer\n",
        "\n",
        "# )\n",
        "\n",
        "# epochs_ry_cnn = 160\n",
        "# train_loader_ry_cnn, test_loader_ry_cnn= create_data(pytorch_train_ry, pytorch_test_ry, batch_size_ry_cnn)\n",
        "# optimizer_ry_cnn = torch.optim.Adam(cnn_test_ry.parameters(), lr = learning_rate_ry_cnn, weight_decay = weight_decay_ry_cnn)\n",
        "# loss_fn_ry_cnn = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "\n",
        "# training_procedure(cnn_test_ry, optimizer_ry_cnn, train_loader_ry_cnn, epochs_ry_cnn,\n",
        "#                    loss_fn_ry_cnn, validation_loader = test_loader_ry_cnn, device = device, mu = mu_ry_cnn, test = True)\n",
        "\n",
        "# __ = evaluating_procedure(cnn_test_ry, test_loader_ry_cnn, loss_fn_ry_cnn, cm = True, device = device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zLE1PyEwfd0"
      },
      "source": [
        "<br>\n",
        "\n",
        "**2.**   Considering the dataset with no repetition and five imputs matrices representing the Red and Yellow positions in the match grid, the matrix of Empty cells and the matrices created via the \"three_consecutive_matrix\" function applied respectively to the Red token grid and the Yellow one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uc0XQBQywk95"
      },
      "outputs": [],
      "source": [
        "cnn_model_ryery = nn.Sequential(\n",
        "\n",
        "    #Kernel a small matrix used in convolution to capture certain patterns\n",
        "    #Padding to add extra pixels around the edges of the input image\n",
        "\n",
        "    nn.Conv2d(5, 64, kernel_size =4, padding=2),     # Convolutional layer with 2 input matrices\n",
        "    nn.BatchNorm2d(64),                              # Normalize the input values ​​in each mini-batch, so that they have a mean of zero and a standard deviation of one\n",
        "    nn.ReLU(),                                       # ReLU activation function\n",
        "    nn.Dropout(0.45),\n",
        "\n",
        "    nn.Conv2d(64, 64, kernel_size=4, padding=1),     # Convolutional layer\n",
        "    nn.BatchNorm2d(64),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "\n",
        "    nn.Conv2d(64, 64, kernel_size=4, padding=1),     # Convolutional layer\n",
        "    nn.BatchNorm2d(64),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "\n",
        "    nn.Conv2d(64, 32, kernel_size=3, padding=1),     # Convolutional layer\n",
        "    nn.BatchNorm2d(32),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "\n",
        "    nn.Flatten(),                                    # Flatten the input\n",
        "\n",
        "    nn.Linear(960, 64),                              # Linear layer\n",
        "    nn.BatchNorm1d(64),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "\n",
        "    nn.Linear(64, 7),                                # Linear layer\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "if device == 'cuda':\n",
        "  summary(cnn_model_ryery.cuda(), (5,6,7))\n",
        "else:\n",
        "  summary(cnn_model_ryery, (5,6,7))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "\\- Model training"
      ],
      "metadata": {
        "id": "e70UdxMWpa05"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOLOMRMzw7cC"
      },
      "outputs": [],
      "source": [
        "# 1. Compute the batch of both train and validation sets\n",
        "\n",
        "batch_size_ryery_cnn=30\n",
        "train_loader_ryery_cnn, val_loader_ryery_cnn = create_data(train_ryery, val_ryery, batch_size_ryery_cnn)\n",
        "\n",
        "\n",
        "# 2. Set hyperparameters and functions for the training procedure such as: learning rate, multiplicative learning rate decay factor gamma,\n",
        "#    percentage of learning rate decay mu, number of epochs, weight decay (regularization parameter), optimizer and loss function\n",
        "\n",
        "learning_rate_ryery_cnn = 0.001\n",
        "epochs_ryery_cnn = 120\n",
        "weight_decay_ryery_cnn = 1e-3\n",
        "optimizer_ryery_cnn = torch.optim.Adam(cnn_model_ryery.parameters(), lr = learning_rate_ryery_cnn, weight_decay = weight_decay_ryery_cnn)\n",
        "loss_fn_ryery_cnn = nn.CrossEntropyLoss()\n",
        "mu_ryery_cnn = 5/8\n",
        "gamma_ryery_cnn = 0.4\n",
        "\n",
        "\n",
        "# # 3.  Training of the model\n",
        "\n",
        "# training_procedure(cnn_model_ryery, optimizer_ryery_cnn, train_loader_ryery_cnn, epochs_ryery_cnn,\n",
        "#                    loss_fn_ryery_cnn, validation_loader = val_loader_ryery_cnn, device = device, mu = mu_ryery_cnn, gamma = gamma_ryery_cnn)\n",
        "\n",
        "\n",
        "# # 4. Evaluation of the model in the validation set\n",
        "\n",
        "# x = evaluating_procedure(cnn_model_ryery, val_loader_ryery_cnn, loss_fn_ryery_cnn, cm = True, device = device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "\\- Retrain the model on the whole training dataset and evaluate it on the test set"
      ],
      "metadata": {
        "id": "kfSTe_5FphxD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RETcYGdCI7xq"
      },
      "outputs": [],
      "source": [
        "cnn_test_ryery = nn.Sequential(\n",
        "\n",
        "    #Kernel a small matrix used in convolution to capture certain patterns\n",
        "    #Padding to add extra pixels around the edges of the input image\n",
        "\n",
        "    nn.Conv2d(5, 64, kernel_size =4, padding=2),     # Convolutional layer with 2 input matrices\n",
        "    nn.BatchNorm2d(64),                              # Normalize the input values ​​in each mini-batch, so that they have a mean of zero and a standard deviation of one\n",
        "    nn.ReLU(),                                       # ReLU activation function\n",
        "    nn.Dropout(0.45),\n",
        "\n",
        "    nn.Conv2d(64, 64, kernel_size=4, padding=1),     # Convolutional layer\n",
        "    nn.BatchNorm2d(64),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "\n",
        "    nn.Conv2d(64, 64, kernel_size=4, padding=1),     # Convolutional layer\n",
        "    nn.BatchNorm2d(64),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "\n",
        "    nn.Conv2d(64, 32, kernel_size=3, padding=1),     # Convolutional layer\n",
        "    nn.BatchNorm2d(32),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "\n",
        "    nn.Flatten(),                                    # Flatten the input\n",
        "\n",
        "    nn.Linear(960, 64),                              # Linear layer\n",
        "    nn.BatchNorm1d(64),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "\n",
        "    nn.Linear(64, 7),                                # Linear layer\n",
        ")\n",
        "\n",
        "epochs_ryery_cnn = 160\n",
        "train_loader_ryery_cnn, test_loader_ryery_cnn= create_data(pytorch_train_ryery, pytorch_test_ryery, batch_size_ryery_cnn)\n",
        "\n",
        "optimizer_ryery_cnn = torch.optim.Adam(cnn_test_ryery.parameters(), lr = learning_rate_ryery_cnn, weight_decay = weight_decay_ryery_cnn)\n",
        "loss_fn_ryery_cnn = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "\n",
        "training_procedure(cnn_test_ryery, optimizer_ryery_cnn, train_loader_ryery_cnn, epochs_ryery_cnn, loss_fn_ryery_cnn,\n",
        "                   validation_loader = test_loader_ryery_cnn, device = device, mu = mu_ryery_cnn, gamma = gamma_ryery_cnn, test = True)\n",
        "\n",
        "__ = evaluating_procedure(cnn_test_ryery, test_loader_ryery_cnn, loss_fn_ryery_cnn, cm = True, device = device)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}